{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7t1uHAMfrBm",
        "outputId": "c276f69b-7a14-43f1-a129-9846eb7b03ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchtune in /usr/local/lib/python3.11/dist-packages (0.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: torchdata==0.11.0 in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.11.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from torchtune) (2.14.4)\n",
            "Requirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.32.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.5.3)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.3.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.9.0)\n",
            "Requirement already satisfied: blobfile>=2 in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.0.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.21.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtune) (4.67.1)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from torchtune) (2.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from torchtune) (5.9.5)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.32.3)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.23.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (5.4.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.11.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (1.1.2)\n",
            "Requirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (0.1.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->torchtune) (4.9.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->torchtune) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchtune"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwGCpc0UYFbl",
        "outputId": "9edaa9dc-0a73-489f-aaa5-4be2e366ffdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2 as cv\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.models import densenet121, DenseNet121_Weights\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
        "from torchtune.modules import RotaryPositionalEmbeddings\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms import v2"
      ],
      "metadata": {
        "id": "C5WvYvKagpNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_video_length = 24\n",
        "num_features = 1024\n",
        "image_size = 128"
      ],
      "metadata": {
        "id": "_fn7BR20_m-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download data\n",
        "\n",
        "To limit the runtime of this notebook, a subsampled version (Paul, 2021) of the UCF101 dataset is used that contains only the 5 most frequent classes. It is already split into train and test data."
      ],
      "metadata": {
        "id": "hizfbTsqgknn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!!wget -q https://github.com/sayakpaul/Action-Recognition-in-TensorFlow/releases/download/v1.0.0/ucf101_top5.tar.gz\n",
        "!tar xf ucf101_top5.tar.gz\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "labels = list(train_df['tag'].value_counts().index)"
      ],
      "metadata": {
        "id": "Q616sEdEhirm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the train data are almost perfectly balanced:"
      ],
      "metadata": {
        "id": "avkHI5Y8rN2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('whitegrid')\n",
        "fig, ax = plt.subplots(figsize=(5, 2.5))\n",
        "p = sns.countplot(\n",
        "    train_df,\n",
        "    x='tag',\n",
        "    order = train_df['tag'].value_counts().index,\n",
        "    ax=ax\n",
        ")\n",
        "p.set_xlabel(None)\n",
        "p.set_ylabel(None)\n",
        "p.set_xticks(range(5))\n",
        "p.set_xticklabels(labels=labels, rotation=77)\n",
        "p.set_title(\"Number of samples per action type\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "BHktqvc-ql9J",
        "outputId": "6e13acb3-d31e-482d-b336-a8125e775605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x250 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAFWCAYAAADwszNCAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAWtRJREFUeJzt3XlcVPX+x/EXOy4soiCIggICLoAgiCsuqWmaa2luqblcTdNb+su6meWSZtniUi5ppWm5r2l6c99NU8QVVPZFRNkXWWbm94d3ThJgQtjA8Hk+Hj1yzpyZ+cyXM+d9zvec8z0GGo1GgxBCCKHHDHVdgBBCCPGsSdgJIYTQexJ2Qggh9J6EnRBCCL0nYSeEEELvSdgJIYTQexJ2Qggh9J6EnRBCCL0nYSeEEELvSdiJIs6dO4eHhwf79+/XdSlP5f79+0yZMoXAwEA8PDz4/vvvdV1Sudq+fTseHh7ExsbquhS91KVLF9555x1dlyGeMQk7HdGuwLy8vEhMTCzy/IgRI+jdu7cOKqt8FixYwIkTJxg/fjyffPIJHTp00HVJooK5ePEiS5cuJT09XdelKCpiTfpMwk7H8vLyWLVqla7LqNTOnj3Lc889x5gxY+jbty+urq66LklUMJcuXWLZsmXFBsv+/fuZO3duhapJlD8JOx1r0qQJmzdvLnbvTt9lZ2eXy/s8ePAAS0vLcnkv8c/RaDQ8fPhQ12VgamqKiYmJrssQz5iEnY7961//Qq1W88033zxxvtjYWDw8PNi+fXuR5zw8PFi6dKnyeOnSpXh4eBAREcH06dNp2bIlrVu35ssvv0Sj0ZCQkMDEiRPx8/OjXbt2fPvtt8V+plqt5vPPP6ddu3a0aNGCCRMmkJCQUGS+y5cvM2bMGFq2bImPjw/Dhw/n999/LzSPtqbbt28zbdo0AgICGDp06BO/c0xMDFOmTKFVq1b4+PgwaNAgjh49qjyv7QrWaDRs2LABDw8PPDw8nviee/fuZcCAAfj6+uLn58eLL77I2rVrledTU1NZuHAhL774ojLP2LFjuXnzZqH30R7X3LdvH8uWLaNDhw74+voyZcoUMjIyyMvL46OPPqJNmzb4+vry7rvvkpeXV+g9PDw8mDNnDrt37+b555/Hy8uLAQMGcP78+Sd+B61jx44xdOhQWrRoga+vL+PHj+fWrVuF5klKSuLdd98lKCiI5s2b0759eyZOnPiXx//eeecdfH19iYmJYcyYMbRo0YL27duzbNky/nyjFLVazffff0+vXr3w8vKibdu2zJo1i7S0tELzdenShX/961+cOHGCAQMG4O3tzcaNG0us4cKFC0yZMoVOnTrRvHlzOnbsyPz584sNyDt37jB16lRat26Nt7c3zz//PF988QXwaNn75JNPAHjuueeU5UTbBsUds/urZQ8KLwPLly8nKCgILy8vRo4cSVRU1BPb90k1DR8+nD59+hT7uueff54xY8YAf6wT1qxZw/fff0/nzp3x9vZm+PDhhIWFFdtG2u+kXdYOHTr0xDr1ibGuC6jq6tevT9++fdm8eTPjxo2jbt265fbeb775Jq6urkybNo1jx46xfPlyrK2t2bhxI61bt2b69Ons2bOHhQsX4uXlRUBAQKHXL1++HAMDA8aNG8eDBw9Yu3Yto0aNYteuXZibmwNw5swZxo0bR/PmzZk8eTIGBgZs376dkSNH8uOPP+Lt7V3oPadOnYqzszNvvvlmkZXm4+7fv88rr7xCTk4OI0aMoFatWuzYsYOJEyeyZMkSunXrRkBAAJ988glvv/027dq1o2/fvk9sj1OnTvHWW2/Rpk0bpk+fDkB4eDgXL15k5MiRwKOV3MGDB+nRowf169fn/v37bNq0ieHDh7N3794if59Vq1Zhbm7O+PHjiYqKYv369RgbG2NgYEB6ejqTJ0/m8uXLbN++HUdHRyZPnlzo9efPn2ffvn2MGDECU1NTfvrpJ8aOHcuWLVtwd3cv8bvs3LmTd955h/bt2zN9+nRycnL46aefGDp0KDt27KB+/foAvPHGG9y+fZvhw4fj6OhIcnIyp06dIiEhQZmnJCqVirFjx+Lj48P//d//ceLECZYuXYpKpWLq1KnKfLNmzWLHjh0MGDCAESNGEBsby4YNG7h+/To//fRTob2miIgIpk2bxuDBgxk0aBCNGjUq8fP379/Pw4cPGTJkCNbW1oSEhLB+/Xru3r3LkiVLlPlu3rzJsGHDMDY2ZvDgwTg6OhIdHc3hw4d588036datG5GRkfz888+8++671KpVCwAbG5tiP/dplr3HffPNNxgYGPDaa6+RmZnJ6tWrmT59Olu2bCnxuz2ppr59+zJz5kzCwsIKLQMhISFERkYyceLEQu+1c+dOsrKyGDp0KLm5ufzwww+MHDmSPXv2UKdOHQBu3brFkCFDqFu3LuPGjaN69er88ssvTJo0iaVLlxb5TnpJI3Ri27ZtGnd3d01ISIgmOjpa07RpU83cuXOV54cPH67p1auX8jgmJkbj7u6u2bZtW5H3cnd31yxZskR5vGTJEo27u7vm/fffV6YVFBRogoKCNB4eHpqVK1cq09PS0jTe3t6aGTNmKNPOnj2rcXd313To0EGTkZGhTN+3b5/G3d1ds3btWo1Go9Go1WpN9+7dNa+99ppGrVYr8+Xk5Gi6dOmiGT16dJGa3nrrradqn48++kjj7u6uOX/+vDItMzNT06VLF03nzp01KpWq0PefPXv2X77nvHnzNH5+fpqCgoIS58nNzS303hrNo7Zv3ry5ZtmyZco0bRv17t1bk5eXp0x/6623NB4eHpqxY8cWeo/BgwdrOnfuXGiau7u7xt3dXXPlyhVlWlxcnMbLy0szadIkZZp2WYmJiVHawd/fXzNz5sxC75eUlKRp2bKlMj0tLU3j7u6uWb169RPbpTgzZszQuLu7F1om1Wq1Zvz48ZpmzZppHjx4oNFoNJrz589r3N3dNbt37y70+uPHjxeZ3rlzZ427u7vm+PHjT1VDTk5OkWkrV67UeHh4aOLi4pRpw4YN0/j6+haapq1Xa/Xq1YXa8HGdO3cutPw/7bKnXQZ69uypyc3NVeZdu3atxt3dXRMaGvrE71dSTenp6RovLy/Np59+Wmj63LlzNS1atNBkZWVpNJo/1gne3t6au3fvKvNdvnxZ4+7urpk/f74ybeTIkZrevXsXqlOtVmsGDx6s6d69+xPr1BfSjVkBNGjQgD59+rB582bu3btXbu/70ksvKf82MjKiefPmaDSaQtMtLS1p1KgRMTExRV7fr18/atasqTzu0aMHtra2HDt2DIAbN24QGRnJiy++SEpKCsnJySQnJ5OdnU2bNm04f/48arW60Hu+8sorT1X7sWPH8Pb2xt/fX5lWo0YNBg8eTFxcHLdv3366RniMpaUlOTk5nDp1qsR5TE1NMTR89LNQqVSkpKRQvXp1GjVqxPXr14vM37dv30J7Lt7e3mg0GgYOHFhoPm9vbxISEigoKCg03dfXl+bNmyuP69Wrx3PPPcfJkydRqVTF1nj69GnS09Pp1auX0ubJyckYGhri4+PDuXPnADA3N8fExITffvutSJfi0xo2bJjybwMDA4YNG0Z+fj5nzpwBHu19WVhY0K5du0K1NGvWjOrVqyu1aNWvX/+pz5bV9h7Ao+O7ycnJ+Pr6otFolL9FcnIy58+fZ+DAgdSrV6/Q6w0MDMr0nUu77A0YMABTU1PlsfZ1xf2mnoaFhQXPPfcce/fuVXo/VCoVv/zyC8899xzVq1cvNH/Xrl0L9Th4e3vj4+Oj/E5TU1M5e/YsPXv2JDMzU/kbpaSk0L59eyIjI6vEOQPSjVlBvP766+zevZtVq1Yxc+bMcnnPP//4LSwsMDMzK9J9Y2FhQWpqapHXOzs7F3psYGCAs7MzcXFxAERGRgIwY8aMEmvIyMjAyspKefxXXWda8fHx+Pj4FJnu4uKiPP+kbr7iDB06lF9++UXpLm7Xrh09e/YkKChImUetVrNu3Tp+/PFHYmNjCwWOtbV1kfcsro0BHBwcikxXq9VkZGQoXVZQtI0BGjZsSE5ODsnJydja2hZ5Xtvu2q7XP9NuoJiamjJ9+nQWLlxIu3bt8PHxoVOnTvTr16/Y9/0zQ0NDGjRoUGiatttRuwxERUWRkZFBmzZtin2PBw8eFHr8tH9/ePQ3XrJkCYcPHy4S1pmZmcAfgVLaZeGvPrc0y96flwHtyVJ/5yzLfv36sW/fPi5cuEBAQACnT5/m/v37xXbVl7QM/fLLLwBER0ej0WhYvHgxixcvLvbzHjx4UK6HUCoiCbsK4vG9u/Hjxxd5vqSt1JK2/gFlD+VxRkZGxc6recLxs5JoX/P222/TpEmTYuf581aomZlZqT+nvNSuXZudO3dy8uRJjh8/zvHjx9m+fTv9+vVj4cKFAKxYsYLFixczcOBApk6dipWVFYaGhsyfP7/YNiqujZ80vSztXNJ7fPLJJ8WG1uN/41GjRtGlSxcOHjzIyZMnWbx4MatWrWLt2rU0bdr0b9eiVqupXbs2ixYtKvb5P29YPb639iQqlYrRo0eTlpbG2LFjcXFxoXr16iQmJvLOO+8U6THQpWfxt27fvj116tRh9+7dBAQEsHv3bmxtbWnbtm2p30vbVq+99lqJe9VOTk5lrrWykLCrQCZOnMju3buLPTNTu3f0563F+Pj4Z1bPn88o02g0REVFKWc8arf6a9asWaYf4ZPUq1ePiIiIItPDw8OV58vC1NSULl260KVLF9RqNR9++CGbNm3i9ddfx9nZmQMHDhAYGMj8+fMLvS49Pb3QHll5Ke6svcjISKpVq1biCRTadq9du/ZTtbuTkxOvvfYar732GpGRkfTr149vv/22xIDSUqvVxMTEFDqJRPs3cXR0VN77zJkz+Pn5PXWQPY2wsDAiIyNZuHAh/fr1U6b/uQta2xbFnX34uNJ0aT6rZa80NRkZGdG7d2927NjB9OnTOXjwIIMGDSp2Y7WkZUj7N9K2kYmJSbn/TisTOWZXgTg5OdGnTx82bdpEUlJSoedq1qxJrVq1uHDhQqHpP/744zOrZ+fOnUp3ETw6PpOUlKR0+zVv3hwnJye+/fZbsrKyirw+OTm5zJ/dsWNHQkJCuHTpkjItOzubzZs34+joiJubW6nfMyUlpdBjQ0NDJbi1lwUYGRkV2SL/5ZdfntkxjUuXLnHt2jXlcUJCAocOHaJdu3Yl7oV36NCBmjVrsnLlSvLz84s8r233nJwccnNzCz3n5OREjRo1ilwGUZINGzYo/9b87xIPExMTpduyZ8+eqFQqvv766yKvLSgoKHNXnnZv6fG/hUajYd26dYXms7GxISAggG3bthXZ8Hv8tdWqVQMedav/lWex7BXnr2rq27cvaWlpzJo1i+zs7BIvRzh48GCh5TMkJITLly8rv9PatWvTqlUrNm3aVOw5AX/nd1qZyJ5dBTNhwgR27dpFREQEjRs3LvTcyy+/zKpVq3jvvfdo3rw5Fy5cKHYLtLxYWVkxdOhQBgwYoFx64OzszKBBg4BHK6R58+Yxbtw4evfuzYABA6hbty6JiYmcO3eOmjVrsmLFijJ99vjx49m7dy/jxo1jxIgRWFlZsXPnTmJjY1m6dGmJXUdPMnPmTNLS0mjdujV169YlPj6e9evX06RJE2XUlU6dOvHVV1/x7rvv4uvrS1hYGHv27Cly7Kq8uLu7M2bMmEKXHsCjSwZKUrNmTT788EPefvttBgwYwAsvvICNjQ3x8fEcO3YMPz8/Zs2aRWRkJKNGjaJHjx64ublhZGTEwYMHuX//Pr169frL2szMzDhx4gQzZszA29ubEydOcPToUSZMmKDsdbZq1YrBgwezcuVKbty4Qbt27TAxMSEyMpL9+/fz3nvv0aNHj1K3i4uLC05OTixcuJDExERq1qzJgQMHig3PmTNnMmTIEPr378/gwYOpX78+cXFxHD16lF27dgHQrFkzAL744gteeOEFTExM6Ny5c5Fudng2y15x/qqmpk2b4u7uzv79+3F1dVXm/zMnJyeGDBnCkCFDyMvLY926dVhbWzN27Fhlng8++IChQ4fy4osvMmjQIBo0aMD9+/cJDg7m7t277N69u1y+U0UmYVfBODs706dPH3bs2FHkuUmTJpGcnMyBAwf45ZdfCAoKYvXq1SWeHPB3TZgwgdDQUFatWkVWVhZt2rThgw8+ULZIAQIDA9m0aRNff/0169evJzs7G1tbW7y9vRk8eHCZP7tOnTps3LiRTz/9lPXr15Obm4uHhwcrVqygU6dOZXpP7THRH3/8kfT0dGxtbenZsydvvPGGsgKbMGECOTk57Nmzh3379tG0aVNWrlzJZ599Vubv8iQBAQG0aNGCr776ivj4eNzc3FiwYAGenp5PfN2LL76InZ0dq1atYs2aNeTl5VG3bl38/f0ZMGAAAPb29vTq1YszZ86we/dujIyMcHFx4csvv+T555//y9qMjIxYvXo1H374IZ9++ik1atRg8uTJTJo0qdB8c+bMoXnz5mzcuJEvvvgCIyMjHB0d6dOnD35+fmVqFxMTE1asWMG8efNYuXIlZmZmdOvWjWHDhhU5ScPT05PNmzezePFifvrpJ3Jzc6lXrx49e/ZU5vH29mbq1Kls3LiREydOoFarOXToULFh9yyWveI8TU19+/bl008/feI1pP369cPQ0JC1a9fy4MEDvL29ef/997Gzs1PmcXNzY9u2bSxbtowdO3aQmpqKjY0NTZs2LfL31FcGmvI4Yi6EKDUPDw+GDRvGrFmzdF1KEe+88w4HDhwo1JUn/nlr165lwYIFHD58uMixwtjYWJ577jnefvttZVQVUTI5ZieEEBWQRqNh69atBAQElNtJMVWZdGMKIUQFkp2dzeHDhzl37hxhYWHFnvwjSk/CTgghKpDk5GSmTZuGpaUlEyZM4LnnntN1SXpBjtkJIYTQe3LMTgghhN6TsBNCCKH3Sn3M7vz586xZs4arV6+SlJTEV199RdeuXQHIz8/nyy+/5Pjx48TExCjDSE2bNq3QIKNdunRRBpLVmjZtWrFjQhZHrVZTUFCAoaFhmUc2F0IIUflpNBrUajXGxsZPvOC/1GGXnZ2Nh4cHAwcOLHIjyocPH3L9+nUmTpyIp6cn6enpfPTRR0ycOLHIHbanTJmijMQBj26h8bQKCgq4cuVKaUsXQgihp7y8vArdaunPSh12HTt2pGPHjsU+Z2FhwXfffVdo2vvvv8/LL79MfHx8oWtFatSo8VS3GSmONr29vLxKHD9QCCGE/lOpVFy5cuUvh3F75sfsMjMzMTAwUO7xpPXNN98QGBhIv379WL16dZGbWj6JdF0KIYR43F/lwjO9zi43N5dFixbRq1evQne8HjFiBE2bNsXKyopLly7x+eefk5SUxLvvvluq95euTCGEEE/jmYVdfn4+U6dORaPRMHv27ELPjR49Wvm3p6cnJiYmfPDBB0ybNu2Jfa5/Jt2YQghRtWm7Mf/KMwm7/Px8/v3vfxMfH8/atWsL7dUVx8fHh4KCAmJjY3FxcXnqzzEyMpKwE0II8ZfKPey0QRcVFcW6deue6u7ON27cwNDQkNq1a5d3OUIIIUTpwy4rK4vo6GjlcWxsLDdu3MDKygpbW1umTJnC9evXWblyJSqVSrnjtpWVFaamply6dInLly/TunVratSowaVLl1iwYAF9+vTBysqq/L6ZEEII8T+lDrurV6/y6quvKo8XLFgAQP/+/Zk8eTKHDx8GKHKzwXXr1hEYGIipqSn79u1j2bJl5OXlUb9+fUaNGlXoOJ4QQghRnirlQNAqlYrg4GBatGhR4jE7lVqN0V9cd6GPqur3FkJUTU+TB6DHt/gxMjRk5o8niLiXputS/jGN7KyYN7SDrssQQogKR2/DDiDiXho345J1XUalUVX3Cqvq9xaiKtHrsBOlI3vDQgh9JWEnCpG9YSGEPpK+GyH+JpVaresSdOLvfG9pM/FPkz07If4m6f4tPWmzsqmqx5fL43tL2AlRDqT7t/SkzUpPNhLKTsJOCCEqEdlIKJuqtz8shBCiypGwE0IIofck7IQQQug9CTshhBB6T8JOCCGE3pOwE0IIofck7IQQQug9CTshhBB6T8JOCCGE3pOwE0IIofck7IQQQug9CTshhBB6T8JOCCGE3pOwE0IIofck7IQQQug9CTshhBB6T8JOCCGE3pOwE0IIofck7IQQQui9Uofd+fPnmTBhAu3bt8fDw4ODBw8Wel6j0bB48WLat2+Pt7c3o0aNIjIystA8qampTJs2DT8/P/z9/fnPf/5DVlbW3/oiQgghRElKHXbZ2dl4eHjwwQcfFPv8N998ww8//MCHH37I5s2bqVatGmPGjCE3N1eZZ/r06dy+fZvvvvuOFStWcOHCBWbNmlX2byGEEEI8QanDrmPHjrz55pt069atyHMajYZ169YxceJEunbtiqenJ5988gn37t1T9gDv3LnDiRMnmDdvHj4+Pvj7+zNz5kz27t1LYmLi3/9GQgghxJ8Yl+ebxcbGkpSURNu2bZVpFhYW+Pj4cOnSJXr16sWlS5ewtLTEy8tLmadt27YYGhoSEhJSbIiWRKVSlfickZFR2b6EHnhSuzyJtFnZSLuVnrRZ2Ui7Pf30PyvXsEtKSgKgdu3ahabXrl2b+/fvA3D//n1sbGwKF2FsjJWVlfL6p3XlypVip1erVo2mTZuW6r30SWhoKDk5OaV6jbRZ6dsMpN1kWSs9WdbKpqztplWuYfdP8/LyqtJbOiXx8PDQdQmVjrRZ2Ui7lZ60WdmU1G4qlarEHZ/HlWvY2draAvDgwQPs7OyU6Q8ePMDT0xOAOnXqkJycXOh1BQUFpKWlKa9/WkZGRhJ2xZA2KT1ps7KRdis9abOy+bvtVq7X2dWvXx9bW1vOnDmjTMvMzOTy5cv4+voC4OvrS3p6OlevXlXmOXv2LGq1Gm9v7/IsRwghhADKsGeXlZVFdHS08jg2NpYbN25gZWVFvXr1ePXVV1m+fDnOzs7Ur1+fxYsXY2dnR9euXQFwdXWlQ4cOvP/++8yePZv8/Hzmzp1Lr169qFu3bvl9MyGEEOJ/Sh12V69e5dVXX1UeL1iwAID+/fvz8ccfM27cOHJycpg1axbp6em0bNmS1atXY2Zmprxm0aJFzJ07l5EjR2JoaEj37t2ZOXNmOXwdIYQQoqhSh11gYCChoaElPm9gYMDUqVOZOnVqifNYW1vz2WeflfajhRBCiDKRsTGFEELoPQk7IYQQek/CTgghhN6TsBNCCKH3JOyEEELoPQk7IYQQek/CTgghhN6TsBNCCKH3JOyEEELoPQk7IYQQek/CTgghhN6TsBNCCKH3JOyEEELoPQk7IYQQek/CTgghhN6TsBNCCKH3JOyEEELoPQk7IYQQek/CTgghhN6TsBNCCKH3JOyEEELoPQk7IYQQek/CTgghhN6TsBNCCKH3JOyEEELoPQk7IYQQek/CTgghhN4zLu837NKlC3FxcUWmDx06lA8++IARI0bw22+/FXpu8ODBzJkzp7xLEUIIIYBnEHZbt25FpVIpj2/dusXo0aPp0aOHMm3QoEFMmTJFeVytWrXyLkMIIYRQlHvY2djYFHq8atUqnJycaNWqlTLN3NwcW1vb8v5oIYQQoljlHnaPy8vLY/fu3YwePRoDAwNl+p49e9i9eze2trZ07tyZ119/vUx7d4/vQf6ZkZFRmWrWB09qlyeRNisbabfSkzYrG2m3p5/+Z8807A4ePEhGRgb9+/dXpvXu3Zt69ephZ2dHaGgoixYtIiIigmXLlpX6/a9cuVLs9GrVqtG0adMy113ZhYaGkpOTU6rXSJuVvs1A2k2WtdKTZa1sytpuWs807LZt20ZQUBB169ZVpg0ePFj5t4eHB7a2towaNYro6GicnJxK9f5eXl5VekunJB4eHrouodKRNisbabfSkzYrm5LaTaVSlbjj87hnFnZxcXGcPn2apUuXPnE+Hx8fAKKiokoddkZGRhJ2xZA2KT1ps7KRdis9abOy+bvt9syus9u+fTu1a9emU6dOT5zvxo0bAHLCihBCiGfmmezZqdVqtm/fTr9+/TA2/uMjoqOj2bNnDx07dsTa2prQ0FAWLFhAQEAAnp6ez6IUIYQQ4tmE3enTp4mPj2fgwIGFppuYmHDmzBnWrVtHdnY2Dg4OdO/enddff/1ZlCGEEEIAzyjs2rdvT2hoaJHpDg4OrF+//ll8pBBCCFEiGRtTCCGE3pOwE0IIofck7IQQQug9CTshhBB6T8JOCCGE3pOwE0IIofck7IQQQug9CTshhBB6T8JOCCGE3pOwE0IIofck7IQQQug9CTshhBB6T8JOCCGE3pOwE0IIofck7IQQQug9CTshhBB6T8JOCCGE3pOwE0IIofck7IQQQug9CTshhBB6T8JOCCGE3pOwE0IIofck7IQQQug9CTshhBB6T8JOCCGE3pOwE0IIofck7IQQQui9cg+7pUuX4uHhUei/Hj16KM/n5uYye/ZsAgMD8fX15Y033uD+/fvlXYYQQgihMH4Wb9q4cWO+++475bGRkZHy7/nz53Ps2DG+/PJLLCwsmDt3LpMnT2bjxo3PohQhhBDi2YSdkZERtra2RaZnZGSwbds2Fi1aRJs2bYBH4ffCCy8QHBxMixYtnkU5QgghqrhnEnZRUVG0b98eMzMzWrRowbRp06hXrx5Xr14lPz+ftm3bKvO6urpSr169MoWdSqUq8bnH9yarmie1y5NIm5WNtFvpSZuVjbTb00//s3IPO29vbxYsWECjRo1ISkriq6++YtiwYezZs4f79+9jYmKCpaVlodfUrl2bpKSkUn/WlStXip1erVo1mjZtWqb69UFoaCg5OTmleo20WenbDKTdZFkrPVnWyqas7aZV7mHXsWNH5d+enp74+PjQuXNnfvnlF8zNzcv1s7y8vKr0lk5JPDw8dF1CpSNtVjbSbqUnbVY2JbWbSqUqccfncc+kG/NxlpaWNGzYkOjoaNq2bUt+fj7p6emF9u4ePHhQ7DG+v2JkZCRhVwxpk9KTNisbabfSkzYrm7/bbs/8OrusrCxiYmKwtbWlefPmmJiYcObMGeX58PBw4uPj5eQUIYQQz0y579ktXLiQzp07U69ePe7du8fSpUsxNDSkd+/eWFhYMHDgQD7++GOsrKyoWbMm8+bNw9fXV8JOCCHEM1PuYXf37l3eeustUlNTsbGxoWXLlmzevBkbGxsA/vOf/2BoaMiUKVPIy8ujffv2fPDBB+VdhhBCCKEo97D74osvnvi8mZkZH3zwgQScEEKIf4yMjSmEEELvSdgJIYTQexJ2Qggh9J6EnRBCCL0nYSeEEELvSdgJIYTQexJ2Qggh9J6EnRBCCL0nYSeEEELvSdgJIYTQexJ2Qggh9J6EnRBCCL0nYSeEEELvSdgJIYTQexJ2Qggh9J6EnRBCCL0nYSeEEELvSdgJIYTQexJ2Qggh9J6EnRBCCL0nYSeEEELvSdgJIYTQexJ2Qggh9J6EnRBCCL0nYSeEEELvSdgJIYTQe8bl/YYrV67kv//9L+Hh4Zibm+Pr68v06dNxcXFR5hkxYgS//fZbodcNHjyYOXPmlHc5QgghRPmH3W+//cawYcPw8vJCpVLx+eefM2bMGPbu3Uv16tWV+QYNGsSUKVOUx9WqVSvvUoQQQgjgGYTdmjVrCj3++OOPadOmDdeuXSMgIECZbm5ujq2tbXl/vBBCCFHEMz9ml5GRAYCVlVWh6Xv27CEwMJDevXvz2WefkZOT86xLEUIIUUWV+57d49RqNfPnz8fPzw93d3dleu/evalXrx52dnaEhoayaNEiIiIiWLZsWaneX6VSlfickZFRmeuu7J7ULk8ibVY20m6lJ21WNtJuTz/9z55p2M2ePZtbt27x448/Fpo+ePBg5d8eHh7Y2toyatQooqOjcXJyeur3v3LlSrHTq1WrRtOmTctWtB4IDQ0t9Z6ytFnp2wyk3WRZKz1Z1sqmrO2m9czCbs6cORw9epT169djb2//xHl9fHwAiIqKKlXYeXl5VektnZJ4eHjouoRKR9qsbKTdSk/arGxKajeVSlXijs/jyj3sNBoNc+fO5ddff+WHH36gQYMGf/maGzduAJT6hBUjIyMJu2JIm5SetFnZSLuVnrRZ2fzddiv3sJs9ezY///wzX3/9NTVq1CApKQkACwsLzM3NiY6OZs+ePXTs2BFra2tCQ0NZsGABAQEBeHp6lnc5QgghRPmH3U8//QQ8unD8cQsWLGDAgAGYmJhw5swZ1q1bR3Z2Ng4ODnTv3p3XX3+9vEsRQgghgGcQdqGhoU983sHBgfXr15f3xwohhBAlkrExhRBC6D0JOyGEEHpPwk4IIYTek7ATQgih9yTshBBC6D0JOyGEEHpPwk4IIYTek7ATQgih9yTshBBC6D0JOyGEEHpPwk4IIYTek7ATQgih9yTshBBC6D0JOyGEEHpPwk4IIYTek7ATQgih9yTshBBC6D0JOyGEEHpPwk4IIYTek7ATQgih9yTshBBC6D0JOyGEEHpPwk4IIYTek7ATQgih9yTshBBC6D0JOyGEEHpPwk4IIYTe01nYbdiwgS5duuDl5cXLL79MSEiIrkoRQgih53QSdvv27WPBggVMmjSJHTt24OnpyZgxY3jw4IEuyhFCCKHndBJ23333HYMGDWLgwIG4ubkxe/ZszM3N2bZtmy7KEUIIoef+8bDLy8vj2rVrtG3b9o8iDA1p27Ytly5d+qfLEUIIUQUY/9MfmJKSgkqlonbt2oWm165dm/Dw8Kd6D41GAzwKTiMjo2LnMTIyorG9FaZGBn+v4ErE2dYSlUqFSqUq0+ulzcpG2q30pM3KRtqtKO10bS6U5B8Pu/KgVqsBuH79+hPne7FxdWhc/Z8oqcIIDg7+W6+XNisbabfSkzYrG2m34mlzoST/eNjVqlULIyOjIiejPHjwgDp16jzVexgbG+Pl5YWhoSEGBlVnC0cIIURhGo0GtVqNsfGT4+wfDztTU1OaNWvGmTNn6Nq1K/Aokc+cOcPw4cOf6j0MDQ0xNTV9lmUKIYTQIzrpxhw9ejQzZsygefPmeHt7s3btWnJychgwYIAuyhFCCKHndBJ2L7zwAsnJySxZsoSkpCSaNGnC6tWrn7obUwghhCgNA81fncIihBBCVHIyNqYQQgi9J2EnhBBC70nYCSGE0HsSdkIIIfSehJ0QQgi9J2EnhBCi1OLi4khPT9d1GU9NLj14hjQajQxnJv4xDx48YP78+Xz88ceYmJjoupxKITg4mB07dhAQEICbmxsNGzbE3Nxc12VVeHl5eYwbNw43NzecnZ1xdnamXr161KlTBwsLi78cuksXKl5FeuTxoNMOUmpgYCABKMqVSqXCyMiIw4cPExYWVmzQPXjwgLS0NFxcXHRQYcV1584drly5QmhoKCYmJlSrVo2GDRvi5uaGi4sLbm5uWFtb67rMCicjI4OsrCw2bdqESqXC0tKSOnXq4OLiQvPmzXF3d8ff3x8LCwtdl6qQPbtnICcnh6NHj1JQUICHhwfu7u66LqnSUqvVJCQkYGJiglqtpk6dOhVyq1EXtCH3/fffY2pqyoEDB6hXrx4LFixQ5tH2LqxatYrQ0FA+++wzHVZc8eTn5xMdHU1cXByXL19mz549xMTEUL9+faysrDA1NcXV1ZVatWoxYMAAGjZsqOuSdUq7zB04cICzZ8/SuHFj+vbty40bNzhx4gRbtmwhPz8fQ0NDnJyceO+992jRooWuywZkz67cqNVqDA0NCQkJYeHChcTHx1OrVi0ePHiAmZkZjRo1olmzZgQFBVWYP35FpW3L2NhYfvrpJ3bv3k1SUhI2NjZ4e3szYsQI2rVrV+W7ibX3crx9+zZHjx4lOTkZc3NzYmNj8fX1xc/PjyZNmqBWq9mzZw/9+vXTbcEVkImJCa6urri6unL06FF8fX355JNPSE9PJzw8nP3797Nr1y4aNWqktF9VX+4AFi9ezKuvvsorr7wCgL+/P/7+/jg5OREcHEzfvn355JNPWLlyJUuXLq0QG6iyZ1cONBoNeXl5mJmZ8fbbb5Oens7w4cOxtLQkISGByMhIIiMjuXjxIkFBQbz33nu6LrlC0249vvXWW0RGRvLKK6/g7+9PaGgou3bt4urVq8yfP5+goCBZ8fxPTk4Ovr6+zJo1i+vXr3Px4kWio6MxNDTE3NwcNzc3Fi1aRL169XRdaoXy+EbquHHjOHDgQJFuy8mTJ+Pq6sq///1vWdb+5/nnn2fgwIGMHz8etVpNfn4+pqamaDQa+vTpw/Lly7l37x4zZszg888/x9vbW9cly55deTAwMMDMzAyA2NhYRo8eTfv27QHw9vZGpVKRnp5OYmIitWrV0mWplYJ2j+XgwYNs3rwZT09PAFxcXOjRowdvvvkma9euxdvbW46n8GiFXa1aNa5evVpoCzotLY1bt24RGxuLl5eXBN0TREdHU6tWLQoKCoA/QhCgXbt2nDt3DgMDA9m44tHG/csvv8yPP/5IQEAAvr6+mJmZoVar+fXXX4mOjsbe3h5ra2vS09OpX7++rksGJOz+tpSUFObPn0+rVq3w8vLipZde4sKFC3Tr1k2Zx8jIiFq1aknQlUJUVBQWFhbKiT1qtRqVSoWJiQkTJkxg9OjRFergty5pV8rGxsaEhoYSGRmJs7Mzbm5uSveSKJ627by9vTExMWHhwoVMmzYNW1tb4NGJPSdPnlTO0FSpVBWiS06XDAwM6Nu3LxcvXmTIkCHUrVuXxo0bY2xszM2bNxk0aBAmJibs2bMHMzMzbGxsdF0yIGH3tyUmJhITE0NkZCTfffcdAHfv3sXY2JigoCCaNWtGzZo1dVxl5WNjY4OXlxcrVqxg8eLFGBoaKiumU6dOYWlpiZGRUaEt8KpIu6dx7do15syZQ2JiorJycXBwYOLEiTRv3lzHVVZsGo2GevXqMXXqVObPn8/gwYPx8fHB0dGRI0eOYGJiwuzZswGq9LKmpVarsbW15euvv+by5cucOHGC8PBwCgoKmDFjBj179uTBgwdER0czePBgXZerkGN2f1NBQQHx8fHcu3ePqKgoYmJiuHPnDuHh4Wg0GgwNDXFwcMDZ2ZmePXvSsmVLXZdcaRw6dIgPP/wQa2trfHx8qFOnDhEREdy+fZtBgwYxcuRI5fheVde7d288PDzo06cPAPHx8Rw4cIBbt26xYsUKvLy8dFxhxXXv3j2WLVvG22+/TW5uLvv37+f06dPcv3+fdu3a0a9fP5ycnHRdZoWSlJREfn4+Dg4OxXbrZmVlkZycjLW1dYXpgZGwK2cajYaUlBQSEhKIjo4mJiaGmJgYgoODefXVV3n55Zd1XWKFlZubi6mpaaEfT2hoKHv37iUyMpKkpCQMDQ0ZNWoUXbp0qfIhp92ru379OqNGjeLAgQOFusrz8vKYOHEiLi4uclJUMbS9Art27eKrr77iv//9r65LqvByc3PZtGkTv/76K/DoEI2TkxMeHh54enri4eFRYXuypBuzHB0/fhwbGxuaN2+OjY0NzZo1AyA7O5v4+Hjs7e11XGHFFR0dzaeffkq7du1o3Lgx9vb2WFpa4uHhgYeHB6mpqRQUFMjd7B+jDbvIyEgcHR2V8NeuxE1NTenQoQM7duzQcaUVk7ZLskGDBrRu3Zq7d+9ib2+PRqNR2lZOSnlE24Ny7Ngxvv/+e/z8/HB1deX+/fvExcURFhbGhg0bcHFxYdmyZRWyzSTs/ibtHzUsLIwFCxYwa9YsoPDZXElJSTRq1KjK74k8SWpqKqGhoZw9e5aMjAzs7Oxo0qQJ3t7eeHl54eLigrW1NTk5OVSrVk3X5VYI2uXL3d2d7OxsFi1axFtvvYWVlRUAmZmZXL58mSZNmuiyzApL+xs9dOgQe/fuxcjIiLFjx+Lg4FDo2FxFW2nr0n//+1+CgoL48MMPgUcX5SclJREXF8fNmzeV48VqtbrCre+kG/Nv0m7xrFy5klOnTrFu3TrgjxDMyclh3bp1VKtWjVdffVXH1VZMf94KjIiI4PTp05w8eZLLly+TnJxMjRo18PDwwM3NjQEDBsiF+X+yd+9ePv30U2xsbGjVqhU2Njbs3r0bU1NTZs2aJe1VApVKxeuvv05sbCzh4eHUrFmTZs2a0bJlS/z9/WnUqJH0yDxm48aNpKSkMHHiRF2XUmqyZ/c3aVfSYWFhuLq6Ao9+QPBoy7tatWqEhYUpW9uiKAMDA8LDw7G2tsba2ppGjRrRqFEjhg0bBjy6XuzcuXOcPn2an3/+mRYtWtCiRYsqfybm43r16kX9+vXZs2cPZ86cITs7my5dujBgwAAZru4JtBuqeXl5JCQkcOnSJc6ePcu+ffv46quvsLGx4fTp07ous0LIz8/n4sWLSm+Bv79/hT0+VxzZsysnP/zwA5s3b2bDhg1YWloq0zMzM3nppZeYMmUKL7zwgg4rrLiSk5MZO3YszZo1w9XVlfr162Nvb0+dOnWoVauWcsG+KF5JZ6Tm5uZK2/2FJx1bSkpKIiEhAW9v7yq9YaVdviIiIpgxYwapqalkZWXh4eFBvXr1cHNzo3Hjxnh4eFToY+oSduXkwYMH9O/fn1q1ajFw4ECaNm2Kra0tn332GXfu3OHHH3+UvbsSJCUlsWzZMuV6RbVajY2NDU5OTsotRBwcHLC1taV27dqVamvyWXl8/NBdu3Zx5MgRjIyM8PX1pUOHDgQEBGBqaqrrMiuF69evk5WVhZmZGZaWltjY2GBubi7t9xhtTERERJCYmEhYWBgRERHcvXuXzMxMkpOT6dixIzNmzKiQJ6eAhF250P5xo6Oj+eKLL4iNjSUrK4vY2Fjc3Nz48MMPK8TYcJVBQUEB165d49y5c1y+fJnw8HCysrKwsrLCwsKCvn37VqgLVXVFG3avvfYacXFxdOjQgby8PC5fvkxkZKSyNb5mzRoZQeUJ1q9fz7fffktycjKGhoZYWlri5uZGYGAg9evXp0uXLlV67/jrr7/mpZdews7Ortjnk5KSiIqK4urVq3h6etK6desKuxcsYVfOMjMzCQ8PJyUlRemOq1Gjhq7LqtDUarWywVDcjyQqKopLly5x8OBB5TiUXEz+6J5ibdq0Ye/evTg6OqJSqcjKyiIpKYno6GjOnTvHuHHjqFu3rq5LrVC0y9qtW7cYM2YMU6dOpVOnTnTp0oU33niDX3/9lcuXLwNw7dq1Kruc5eXlMXbsWJYuXYqVlRWvvvoqjo6OtGjRAh8fHzw8PCrkHlxJJOzKQWZmJnfu3FFu5+Pk5ISNjQ3Vq1evVAtDRaDRaNi6dStRUVG4urry/PPPU716dV2XVWFkZmYq3bgFBQX85z//YerUqTg6OhbpPiooKKjy4zgWR7uh9M0333Dy5EnWrl3Lrl27+Pbbb9m1axfXrl3jm2++YerUqTRq1KjCdss9awUFBcTExNCoUSPS0tL4+OOPSUlJITExkZycHKpXr06DBg1o2rQpLVq0IDAwUNclP5H8Ev6m/Px8Pv74Y7Zu3YqVlRW1atXC2toaR0dHGjVqRN26denbt6/0/z+BdmVy//59Pv/8c65cuQLA6tWrCQoKwsTEhF9++QU/P78KM4K6rixZsoR169YREBCAn58fGRkZrFu3jnfffbfIClmCrnjadrpz5w6NGzcG4OrVqzRt2hSAZs2aYW5uzsGDBxk3bpzO6tQ1Y2NjGjVqBICVlRXz5s0jISGBmJgY4uLilNGh9u7dy5UrVwgMDKzQGwbyaygjbb/06dOn+fXXX/nhhx9o0qQJly9fJjg4mBs3brB//35MTU1liLC/oL0Add++fYSHh/PVV19x7do1Vq1aRe3atUlNTeXkyZPEx8czYcIEXZerUz179sTBwYHbt29z6tQpUlJSOHLkCEePHiUgIIA2bdrg7++vdF1W5JWPrmi7ylu2bEl2djYANWvW5M6dOyQlJWFgYMCFCxfo0KEDULXbULsXvGXLFuzs7OjYsaOywalSqUhOTiY2NlYZ6KEit5WE3d90584dOnfuTEBAAPDo3lft2rUDHu31xcfH67K8SuXo0aMEBgbi5OTE4sWLldH6tfesS0tLA0o+1b4q8PX1xdvbm+zsbFJTU0lMTCQ2NpYbN25w/fp1jh8/zsOHD8nLy2PHjh3KlrkoqkePHsTFxQGPBtLeuHEjU6ZMISsri2rVqin3pKyIJ1v8U7S/s/Pnz3Po0CFatmzJ4MGDadWqFRYWFtja2iq3Q4KK3VYSdmWk3XpxcnIiLCyMuLg4HB0dC81jYmKCs7OzLsqrVLQ/qGrVqinXKF69epUpU6Yo81y+fJmpU6cCVXv4Jm3QW1hYYGFhQYMGDfD396dHjx4UFBSQmJhIREQEYWFhsuz9BQsLC+XGwK6urixbtozdu3crJ2bIpUJ/+OSTT7hw4QLff/89K1as4MyZM/Tp06dSnWUuYVdG2hXu5s2bOX78OOnp6fTu3ZuGDRtia2uLlZWVHKcrpYEDB/Luu+9Sr149EhMTlRXRzp07ycrKonXr1kDF3np8VrTd5kePHmXPnj288sorSnvAowvIo6KicHFxwdXVlY4dO1bJdiqNixcvcvXqVczNzfH396dly5ZyC65iaJc97Y2Ad+/ezcaNG9m/fz/t27fnpZdeqhSXt0jY/Q1ZWVk4OTnRrVs3rl+/ztWrV5WLoV1cXHBxcVHuLyb+WqdOnRg/fjzffvstRkZGrFu3jvDwcCIiIpgwYUKFueOxLm3fvh1bW9tCe21Lly5l27ZtmJmZ4efnx9SpU2U8xxJojylt2LCBTZs2YW5uTkhICAsWLMDFxYULFy5gY2ODi4uLrkvVOW0vwtGjR7l06RL379+nWrVqODk5ERgYSFxcHDt37lSCsKJeX6cllx6UE7VazdWrV/n999+5fPkyly9fxsXFhTVr1ui6tAovMjISOzs7qlevTmZmJkePHuXUqVNER0fTtGlTAgMD6dq1q67LrBD8/f1ZvXq1MrDznj17mD9/PkOGDKFOnTosWrSI4cOH8+9//7tCr3h06eHDh/Ts2ZPx48czZMgQAgMDWbNmDc2bN2flypX8/vvvfPrpp1W+G1O7YTBgwACuX79Ot27dcHR05OHDh9SpUwcHBweSk5Px9/fH19e3wh9Llz27Mnj8TEwDAwMaN25MnTp18Pb2LtSHnZqaqrsiK4ljx46xc+dOJk2ahJubGzVr1qR379707t1b16VVOBEREZibmysDjufm5rJz5066deumHN80MTFh27Zt5Obmyq2Q/kT7u71w4QIAQ4YMISQkBENDQ+VEnhYtWrBjx44qH3Twx6Ga//znP+zYsYOoqCh8fHzo3bt3sT0HFTnoQMKuTLRbzN9++y1paWnY2tpSt25dnJycaNy4MQ0bNsTBwUE5i1AULy0tjVWrVtG5c2fc3NyAR1uTarUaeNSNsmXLFvr27SvjYfJo/FV7e3vi4+Px8PDg2LFj3Llzp9DlGHZ2diQnJ0vQFUO78r53755yMllISAiurq7KKEdXr15VTpKq6Hsq/xR/f3+aNGnChQsX2LlzJ2fPnsXX15d+/foVOSmvIpOwKyONRsOIESOIiIjg1q1bREdHExISQkpKCo6OjtjZ2TF//vwqPa5eSbTdI6dOnSInJ6fQff4MDAyUFUxOTg7nzp3DyclJueapKvPy8qJGjRpMnz6d5557jgMHDtC2bVvlsheAs2fPylmYJdCGXZcuXfjuu+/4/vvvOXLkCD4+PgDcunWLX3/9lU6dOumwyopBuxeckJCAqakpRkZGNGzYkKFDh3Lp0iVOnjzJ1q1beeONNxgwYICuy30qEnZlZGBgQMeOHenYsSMA6enpREZGsmXLFg4fPoyrq6sEXQm0YXf69GkaN25c4lmr5ubmODo6cvr0aTp06FChL1j9J5iZmTFr1izWrFmjXJM4fvx45fmQkBAuXbrESy+9pMMqKz5ra2veeustNmzYwLVr1zAzM2PatGmcOXOGDh060K9fP6BqnvX7Z4sXL2bnzp3UqFEDZ2dnzMzMiI6O5sGDBwCkpKQAlWNouopdXSViaWmJt7c39evXR6PR0L17d12XVGFpAysjI4PatWuXGGLGxsZER0crlyBoR1qpylxdXXn11VextrYuctzkv//9L46OjrJn8hQ6d+6MmZkZZ86c4ebNmxQUFDBixAheffVVpUuzKm9YaYO+T58+vPbaa2g0GuLj4zExMcHR0RErKysSExOV0VQqw+9Swq6UtLv3+/btw9LSEhcXF+rWrav8sW1sbLh9+7YyMoMoSrsSad++PcuXL2f8+PHY29ujUqnQaDQYGhpiaGhIYmIily9fZuTIkYVeV5WdPHmSLVu2sHjx4kLHlFQqFW3atKF+/foV+gaaupScnIypqaly/Ldt27a0bdsWjUZDfn6+XBf7P1lZWdy6dYs6derQtm1bZbqHh0eh+R6/FKgy/DYl7ErJ0NAQtVrNxo0bqV69OmZmZtSpU4cGDRpQp04d7ty5Q3R0dKW4yFLXAgICWL58OYsWLeLDDz8sdBJKZmYmq1evxsbGRmnLqtitpN3rTUhIwMHBgcOHD5OXlwf8sTWt7UKyt7fnwYMHcszuMdqN04sXL7Jq1SrOnDmDq6sry5cvJysri+DgYHJzc+nYsSP16tXTdbk6pW2rI0eOcOTIEQYNGqTsuaWlpXHq1CkKCgpo3bo1dnZ2le4EHgm7Urp58yZ79+5VLiswMzMjLCyM4OBg4FHX3KhRo2RMwqfQsGFDpkyZwqxZszh06BDNmzfHx8eH6tWrc+TIERISEvjoo48wNDSs8sfrvvjiC1JSUrh27Rrt2rXj2rVrODg4YGNjoxwrmTt3Lu7u7vj5+em42opDu4E0f/587Ozs+Oijj9iyZQurV6/m7NmzREVFkZeXR/369Vm6dClNmjTRccW6t337djw8PJS2yM7O5qOPPuLEiRNkZ2djamrKggULKt21rxJ2T0G7xbN//36++OILUlNTcXFxITk5mf79+9O/f39u375Namoqjo6OODg46LrkSqNfv364u7uzefNmQkJC2LVrF7m5ubRs2ZK3335bOdOwqgad9nu7ubkRExPDiRMnCAsLY/bs2VSvXp26devi6upK9erViYiIYNKkSTquuOK5desWMTExrFmzBisrK+zt7Rk+fDgTJ05k4sSJREdHM2nSJPbv34+Hh0eV7EGAPzYMgoODefPNN5VLMLZu3cqlS5d45513aNOmDe+99x779u2jdevWleqSIAm7Uli1ahVdunThpZdeIj8/n2+++YYvv/wSb29v5ToxUTrZ2dk0bdqUWbNmkZSURFpaGk5OTpibm+u6tApl/Pjx3Lt3D5VKRa9evYiOjiYiIoK4uDhOnDhBamoqQUFB0n3+GG1vwNGjR3F1dVUuFE9NTaVevXpMmDABU1NT3NzceOmllzh48CBvvvmmjqvWraioKCwtLZXuy5ycHPbs2UOvXr3o27cvAGPGjGHu3Lm6LLNMJOyegqGhIXl5eYSFhbFs2TKlb3/+/Pn4+fkpB7YrWx+2riUnJzNnzhzatm2Lq6srDRs2xMXFhT+PYFfVuzDh0XE5Ozs75s2bh6GhoXIbqfT0dOLj4zEyMlJuRCoe0Z69+9tvv1G/fn3y8vIwNTXlt99+IzAwsNClQRkZGcogEFX5d5yfn4+trS0///wzI0aMYM+ePdy9e5devXop8+Tm5pKRkUHNmjUr1W9Twu4vaP+Yv//+O6ampoUOYufn52NmZkatWrVQqVSV5o9eUaSmpnL48GH279+PoaEhNjY2NGnSBH9/fzw9PXF3d8fBwUHalT/uOr5p0ybOnz+PoaEhgwcPJiAgQOluEoVpA+vu3bvcvXuXmTNn4urqyi+//MKLL75IcnIylpaWGBsb89tvv1X5y4U0Gg1ubm60bduWDRs2cOrUKa5evcrAgQOVDSmVSsWJEyfw8vJSHlf06+u0KkeVFUBYWBh5eXlMmzaNOnXqEBAQwI0bN3B0dKxU/dYVgXbLOTExkSFDhmBubk5QUBA3btzgyJEjfPHFFxgbG2NhYUGvXr2YMGGCnE4PzJkzh/Pnz+Pn58emTZuUlfOBAweoXr06rVu3xsTERMdVViwajYbp06cTEhJCWFgYhw8fxtDQkLNnzxIdHY2DgwNOTk4EBwczb948oGqe9Qt/HB8eNmwYJiYmBAcH884779ClSxdlntOnTxMaGsqgQYMKvaYykLsePKXo6GiOHj3KjRs3uHv3Ljk5OSQmJiojqbi4uNCwYUP8/PyUi1JF8bQn/AQFBfHOO+/wwgsvFHp+3rx5mJiY0KhRI7744guGDh3KG2+8Uam6TMpbYmIi/fr1Y+XKlXh7e+Pv78+OHTto0KABGzZsYP/+/axcuZLq1avrutQKLS0tjdu3bxMSEkJoaChxcXEkJyeTkZHB8ePHdV1ehff2228rI85UtrF/Zc/uKTk5OSljOD58+JCEhARu3brFb7/9RkxMDLdu3eLu3bt89tlnylh7oniGhobk5uaSk5Oj7BUXFBSgVqsxNTVl3LhxTJkyhddff52srCx27tzJSy+9VCXPctVuGPz222/Y2tri7e3NwYMHsbS0VEZQqVevHklJSRJ0xdC2X3JyMjVq1MDKyqrQTVrz8/O5desW6enpheavirQbk1FRUVy8eJE2bdooy5hKpSItLY1PPvmEhw8fVsoTyCTsysDc3JxGjRrRqFEjunfvTkFBAcnJydy6dUsZ2ko8mUqlomPHjqxZs4a2bdsW6ve/du0aoaGhWFhYEBQUxDfffIOdnZ0Oq9Wdx/dktT0GoaGhNGvWTOmy/O2332jQoIFO6qss1q5dy927d2nSpAn169enfv36ODo6YmFhQdOmTZX5qmrQwR8n9OzYsYMLFy7QvHlz7O3tOXz4MJs3b8bIyIixY8fi6+tbKXtZJOzKgbGxMXZ2dlV2hVwW1atXZ+jQobz11lu0a9cOf39/3N3dycrK4siRI7z44ovAo1H8LSwsquzZcQYGBmg0Gnr27Mn69euZNWsWZ8+eVU4D37t3LydPnuS1117TcaUVkza87t69y8GDB9m9ezfOzs7Y2tpia2uLi4sLTk5O+Pj4yMgz/3P06FFeeeUVGjduzM2bN1m2bBl2dnYkJCSwdOlSPvnkk0p5DF3CTuiEWq3Gz8+PnTt3snPnTq5cucKJEyfIzc1lxIgRDB06lMTERK5fv66s2KsqAwMDjI2NmTdvHkuXLiU/P58DBw6wa9cukpKSGDduHN26ddN1mRWOdu8jMTERKysrevTowfDhw7l9+zbBwcEcOHCAU6dO4ezszI4dO5gwYQKBgYG6LltntBuUSUlJynXDmzZtwtnZmc8//5zs7GwGDBjAgwcPKmXYyQkqQmdyc3OBR0OuPX7MTuvevXtER0fj4uJSaNDZqkJ7/Cg8PJzIyEi6dOlCTEwMwcHB3L59GysrK9zd3Wnfvr2uS62QtGf9fvXVV5w7d45vv/22UHd5WFgYc+bMoXv37hw7doywsDB27dpVJZc17YZBVlYWs2bNAqB3797MmDGDxYsX06ZNG2JiYujduzcXL16slD0tsmcn/nEPHz7k8OHDnDx5Eo1Gg7W1tXJR+ePDrUnX8CMHDhzgzJkzdOnShQYNGhQ6PqdSqYiIiJCxWIuhPaYUGxuLtbW1soLOy8vDyMgId3d3nJ2dqV27NsuXL+fll1/m559/LnQzYX2nDTltW9WoUYO+ffvy3nvvcfLkSfr370+bNm3Izs5m7969uLq6YmRkVClP5Klc1YpKTaVSAbB//36+/PJLQkNDycrK4saNG2zcuJHPPvuMt99+m6+//hp4tGdT1anVaoYOHUpcXBx79+4t9NydO3cYOXIkp06d0lF1FZt2ZdyxY0cuXrzIrl27AJQ7byckJHDmzBnl9j5mZmZV7jY/BgYGXLx4kZycHGVaUFAQJ06c4OjRo0ybNg2A27dvExYWptyVvDJ2CMqenfjHbdu2je7duzN9+nTg0bBh4eHh3Lp1i4sXL2JrawtUzh9UedKurK2srHjjjTdYsmQJQUFBGBsb88MPP/Ddd9/Rtm1bgoKCdFxpxdajRw8uXLjAf/7zH7799ltatmyJtbU1+/btw8bGhq5du3L37l0iIyOVgcerkoULF/Lll19SrVo1Vq1aRb169WjWrBkNGjRQun09PT2ZPn26Mr5oZezGlGN24h83e/Zs/Pz8lDMu/6wydpGUp6ioKA4cOECTJk1wdHRUBuWdMWMGderU4fbt20RFRfHuu+/KiSl/4fFl6cqVK/z888+EhISQmZlJ165dGTp0KHXq1GHr1q3s27eP7777TscV/7Oys7P573//S79+/UhLS2PMmDEYGRlhYGBArVq1aNCgAY0bN6ZZs2Y4OTlV6tGiJOzEPyonJ4cvvviC27dvM3v2bOrXr1/prtd51rZt28aSJUtwcHCgZs2a2Nra4uHhwc2bN9m5cycvvvgic+bMoVq1aroutULTHo/KycnBwMCgyIXQ2ufz8vJITEzE2Ni4Sg5coKVSqbh58yZRUVGEh4cTGxtLUlISWVlZPHz4kMaNG/Ppp5/quswyk7AT/wjtmXHnzp1j2rRpZGRk4OrqSkBAAE5OTjRq1Ij69evj4OBQ5cd3VKvVhIWFERYWxpUrV4iMjCQ1NZWcnBwMDQ1xc3PDxcUFKysrOnXqJBeUF0MbZBcvXmTHjh2Eh4djYGCAp6cnfn5+tGjRosrfmRyefEcRtVpNdHQ0UVFRXLt2DXt7ewYMGFBp7wohYSeeqT//mDIzM7l06RLR0dGEhIQQGxtLdna2cruVl19+mYEDB+qq3AorISFBCb+wsDDu379PbGwsixcvxtfXV9flVUi5ubm8+OKLWFpa0qZNG7Kysrhz5w4JCQnk5eVhYGDA1q1bqV27tq5LrTCOHj1KRkYGzZo1w8XFRdfllCs5QUU8UwYGBvzyyy907twZc3NzatasSYcOHYBHo6vn5uYSFhbGzZs3OXv2LHXr1gXkuB1AVlYWt2/fxtnZGQcHBxwcHOjYsSPwaKPh2rVrNGvWTMdVVjzaDayzZ8+Sn5/Pli1bMDAwIDMzk5SUFBITE4mOjiY+Pl6C7n/y8vKYNWsW586dw9jYmNjYWMzMzGjcuDFt27aldevWtGnTRtdl/i2yZyeeqfv37zN58mR+/PFHCgoKmDdvHh4eHri7u+Pq6lolL+D9K9puol9//ZXly5fz9ttv07p1awoKCjA2NiY/P18ZVUWULDg4mF27djFt2rRiT6zIz8+XLvP/bVSeOHGCd999lxkzZtC0aVNSU1O5desWISEhXLx4kbS0NM6cOaPrcv8W+bWIZ8rMzIypU6diaGhIbGws165d486dO+zevZvq1atTt25dGjVqhJubG56enjg6Ouq6ZJ3Tbn/u27ePFi1a0Lp1a+CPSxHu3bvHd999R+vWrenatavO6qyotCvwS5cuce7cOQ4cOFBs13hVDzr448L7K1euEBgYWOgMaT8/P/r3709aWpoy2lFlHABaS/bsxD8qKSmJsLAw7ty5o3QlZWRkcP/+fTp16sSMGTMq9Q+qPGi/f9++fRk0aBDDhg0rdIG9oaEhgwYNYvDgwXJ8808eX3YmTJhAREQEcXFxuLu707RpU7y9vfHx8aFx48ZVvpsc/miviIgI1q1bx7/+9S/ltj769juUsBPPlEajUW4dsmnTJrp161ao6zItLY3o6GiuXbumnJ0px+setducOXNITk5m8eLFhZ5LTEzkhRdeYPPmzbi6uuqowopNrVZz7do1kpOTiY2NJTIykpiYGO7du0dWVhbVq1dn+/bterUy/zs6derE3bt36dWrF0OGDMHb21vvRpORsBP/mAEDBpCSksKECRPo2bMnlpaWui6pQjtz5gzjxo2jT58+vPDCC9StW5f8/Hy+//57bt68ye7du3VdYoVy+vTpImOHamVnZ3Pv3j1lpBSVSqXsMVf1Davs7GzWr19PQkIC586dIzExEUNDQxwdHWnRogVt2rTh+eef13WZf5uEnfjHpKSk8N1333H69GnatWvHpEmTlK3Hqn6yQEldRvv27WP58uVoNBpq1qxJVFQUDg4OzJ49Gy8vLx1UWnGNHTuW559/npdffplFixYpo6S0aNGiyAkq+tZF93fl5+eTmZlJdna2skFw8+ZNfv/9d2rVqsWaNWsq/YaBhJ34R2VlZbFr1y5WrVqFqakpU6ZMoXfv3rouq0IICQnBy8uryEo4Pj6e4OBgYmNj8fPzo2nTplSvXl1HVVZcERERODg4YG5uzkcffcS5c+d48OABDx8+xM7ODh8fH1q3bk3Lli1p0KCBBN5fyMvLIyUlBYC6detW+vaSsBP/mMd/LDk5OXz11VdcuHCBVq1a8dJLL+Hk5FTpf1Bl9fDhQ/z9/Tl+/Dg2NjbMmzePVq1a4e/vL5dnlNG9e/eIjY0lNjaWmzdvcuPGDaKiokhMTOTs2bNYWFjousQK4caNGyxZsoTU1FRcXFzw8fGhWbNm1K9fXxn4WR9I2Il/RH5+Pjk5OQQHB3P//n3UajW///47O3bsAKBdu3ZMmzaNpk2b6rhS3SgoKCA2NpaGDRuSnJzM2LFjiYuLIy0tTdkradOmDW3atJF715VAex3i+fPnuXTpEuPHj1eee/jwIRkZGSQlJZGSkkK7du10WGnFkZSUxCuvvEK9evXw9PTk+vXr3Lx5k6ysLOrWrYuLiwsrV67Ui5NVJOzEM5ecnEyfPn14+PAhrq6upKeno9FoaNeuHe7u7hgaGnL06FGOHTvG559/Tvfu3XVdsk5FRETw8OFDLCwsiImJ4cqVK/z++++EhoZy9+5dWrZsyYYNG3RdZoWj7RX4+eef+eyzzwgKCmLkyJF6N+xVedAef9u+fTvff/89P/74Y6HjmuHh4Zw+fVq5m7s+9LhI2IlnLjo6muPHj9O8eXMKCgrw9PSkZs2a5OXlFdpiXLx4MRcuXOCHH37QYbW6o90zeeutt2jTpg0vv/yy8lxeXh4ZGRlERUVhaGhIixYtdFdoBZednc3JkydZuXIlBQUFvPrqq3I9Ygm2b9/O1atXmTVrFqDfw/Tp57cSFYqTkxPDhw+nRYsW+Pv7K1uQpqamFBQUkJeXB4ClpSWRkZE6rFS3tMN/3bp1q8iYjaamptSuXVsZsV+UrHr16nTv3p1Vq1YRFBTE/PnzGTx4MMeOHaOgoEDX5VUI2n0ca2trrl+/zt69e9FoNIWCTt/2g2TPTjxzGo2GnJwcbt++jUqlwt7eHjs7u0K3CdFoNOTn53P37l2cnJx0WK1uaa+jS09PZ8qUKVX6coyy+vPeSVxcHD/88ANhYWH07t2bAQMG6LC6iiMvL4+AgAByc3OpUaMGrVu3pnXr1vj4+ODk5ISVlVWl77p8nIyNKZ4ptVrN7t27mTdvHvb29piYmGBnZ4ejoyONGjXCzs6OLl26YGJigqmpaZUNOu0K+ujRo3z22WfAo27Nzp0706BBAywtLTE3N6+U9xH7Jzx+TCk7O5srV66Qm5uLRqPhxo0bBAcHExwczMOHDyv1PdnKk6mpKadPnyY6OpqzZ89y9uxZVq5cSXJyMubm5rRq1YoVK1bousxyI3t24pnQrrzPnTvHe++9x7/+9S9MTEyYOXMmzz33HCdPnsTMzIxmzZrxzTff6LrcCuPevXusXbuWsLAwzp8/z8OHD3FwcKBp06a4u7vTo0cPPDw8dF1mhaNd3kaPHs2VK1dwdXXl3r175OTk0Lx5c5ydncnPz6dbt2506NBBwq4EKpWK+Ph4zp49S2ZmJqNHj9abtpI9O/FMaLehDh48SJMmTXj55Zf56aef8Pf3Z/HixRw7doxly5bRv39/QL8PjD+Nc+fOsWDBApYvX87//d//KdPDw8M5efIk169fZ/ny5TRo0EDCrhjaZWfo0KHKSU+enp7UrVuXtLS0IteL6cPK++8qKCjg1KlTnDhxgrS0NCZMmICrq2uRIdf0pa0k7MQzoe1SCg8PJzAwEIBTp04pNxtt27Ytu3btokaNGoXmr4pSU1NZsmQJL774Ira2thQUFGBgYICRkREuLi64uLhw69YtPv7440J3PxBFdevWrcg0KysrCgoKUKvVenG9WHlZunQpmzZtokePHuzZs4f+/fvj6urKrl27sLe3x9/fX2+CDuRsTPGMaLe0mzVrpqxgatasSUpKChkZGajVas6ePYu5ubkuy9Qp7d7vmTNnyM3NZciQIRgbG2NsbFxoJRMfH8+kSZM4d+5cld77fRqZmZkcO3aMX3/9lZiYGGW6sbGxBB0oG0vXr19n165drFu3jpEjR2JpaYmnpycajYbMzEx27NihV0EHsmcnngHtbX0ARo0aRWpqKgA9e/Zk9uzZLF68mKtXr1KzZk0CAgKAqrlnpz2p4vjx47i6upY43mXdunV5/vnnOXr0qLKXLP6g7QI/c+YMn376Kaampty7dw+A2rVrK0Nf9ezZs8rfHFi7sXTy5EkcHR1xd3dn586d2NvbK8PSaTQa5RIgfTq8oB/fQlQYWVlZSheckZERNjY2yggWLVu2ZMCAAcoJBJ999hmGhoZ6dz3P09IGfGZmJnZ2duTn5wNFr28yMjIiIiJCbolUAu3KeOHChbRs2ZLZs2eTl5dH69atqV27Nhs3bmTdunUkJiYC+nf92NPQbnzu3r2b9PR0TE1Nsba2Bh71LDx+7WZwcLAyJJ0+dZtL2Ily9cYbbxAYGMjkyZOV+65p1axZk8mTJ7Np0yZmzpyp3KKmKu7VwR/fu2XLlhw4cEC5pu7P7ZGSkkJwcDAtW7b8x2usLBISEoiKimLKlCl4eHiQm5vL1KlTWbx4MT179mTgwIFVennTbhDMnTuX5ORkunbtyu3bt9m7dy/BwcH4+/sDcOjQIW7cuKEc+9SXvTqQbkxRzoYOHcqNGzeIjIxk3759fPPNN6hUKlxdXWnZsiWBgYH4+flRrVo1XZdaYXTo0IHPP/+cqVOnMnnyZJycnDAxMVFWNMuXL6dWrVrKCkn8QdvNdu3aNdzd3bGwsODMmTPY2NhgamqKmZkZHTt25PDhw1X+Av3Q0FAePnxIvXr1MDU15f/+7/+YO3cuiYmJbNq0iSNHjnDkyBHGjh1L+/btAQk7IUrUtWtX2rVrx5EjR0hKSsLKyors7GxCQkI4efIkmzdvRqVSYWNjw65du6r0CSrwqEvN1dWVefPm8f777xMaGkrbtm1xc3MjJSWFo0ePEh8fz/vvv69XK57yom0TU1NT6tatS1RUFAYGBsog2rVq1eLq1atkZWUB6M01Y6Xx+LFhPz8/5UQd7Y1tT548yaFDh1CpVCxevJigoCC93PuVi8pFudD+oCIiIvj66685deoU5ubmWFlZMW7cOHr06EFcXBxJSUmEh4eTmJjIpEmT9GI09b9L2wYXLlxgy5YtXLx4kbi4OMzMzAgMDGTIkCF07NhR12VWeLm5ucr/J02aRGpqKubm5ty7d48ZM2bwwgsvVMmw037nN998k0OHDjFixAh8fHyq3L0SJexEudB2J73//vvcuXOHjh07UqtWLX788UcKCgqUC6K18vPzq3y3UkkyMjKUU8Dr1Kkjp8yXwZ07d9i6dSvp6el06tSJoKAgzMzMdF2WTmg3plq3bk2LFi3IysoiIiKC7Oxs5WzVVq1a0bp1a72+HZKEnSgXj/+gPvroI5577jng0b3sevTowZIlS2jdunWV3LIWz97Dhw8pKCggJSUFMzMzbG1tMTAwkOXtf5KSkujUqRNHjx5FpVKRlJRETEwMYWFhXL9+nfDwcNLS0sjNzeXQoUPY2trquuRyJ8fsRLkwMDAgKSmJzMxMJegAbGxsyM7Oxt7eHtCvA95Ct7RBdu3aNZYtW8aRI0eUu2t7enrStGlTHB0dadCggV6uvJ+GdiP0xIkTODs7K+1gb2+Pl5cXXbt2JTMzkwcPHhAXF0dsbKzetpWEnfjbtD+oy5cvU1BQwOrVq2nQoAF+fn4kJCRgbW1Nw4YNgap52rd4NtRqNUZGRsydO5eaNWuyYsUKMjIyOH36NMeOHWP79u2kpaXx8ccf069fvyp5fFj7fSMjI5Wh+lQqFYaGhhgYGGBqaoqNjQ02NjY0btxYr66r+zPpxhTl5vTp06xcuRIDAwOysrKoVq0aycnJpKWlMX36dBo3boyzs7MyHqYQ5aF9+/asXr0aT0/PQtOzsrI4f/48np6e2NvbV8mwe1xV//4SdqJcqdVqMjMziY6OJiQkhODgYBISEsjPz8fIyIjc3Fz+/e9/K9fxCFFad+/eJSwsDBcXF4yMjFixYgVdu3alQ4cOui5NVGASduKZe/jwIXfv3uXmzZscO3aMCRMm4OzsXOW3NEXZrFixgi+//JLatWvj5eVFTk4OqampzJw5ExcXFywsLOQMVlGEhJ0QotLQbiAlJiZy6dIlfv31V65fv05ERATGxsa4urri7e1Ny5YtlXv/1axZU9dliwpAwk4IUenEx8djZmZG7dq1gUcXkl+6dImQkBBOnDjBtWvXyM7OZt26dbRq1UrH1YqKQM7GFEJUKgkJCcydO5f27dszbNgw1Go1ZmZm2Nvbk5mZSd++falbty7Xrl2jcePGui5XVBASdkKISkHbhXnkyBFSUlJo06aN8tzWrVv59NNPMTIyIj8/n5EjRzJ58mS9PpVelI5c4SuEqBS0wXXo0CECAwOVoa3Onj3L1q1b6d69O1u2bGHw4MHs3buXmzdvyiAGQiFLghCiUtAO+3Xv3j3l3nQAO3bswN7ensmTJ+Po6Mjo0aOxsLAgPj4eqJo3axVFSdgJISqNvLw83NzcOHz4MAUFBcTGxnLgwAG6d+9OnTp1ADA3NycqKkoZtUcIkGN2QohKxNTUlD59+jBjxgwSExO5d+8eHh4evPDCC8o8ly5dwtDQUOnmlGs5BcienRCikgkKCmLOnDnUqFGDoKAgFi9erDwXGRnJvn37lBF6VCqVrsoUFYxcZyeE0Bs7duxg//79jBs3Dn9/f+U+i0JI2Akh9Ip2HFYJOfE4CTshhBB6TzZ9hBBC6D0JOyGEEHpPwk4IIYTek7ATQgih9yTshBBC6D0JOyGEEHpPwk4IIYTek7ATQgih9yTshBBC6L3/B+o8CVqTgZiTAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define feature extractor\n",
        "\n",
        "A pre-trained image classification model will be used to extract features from the video frames\n"
      ],
      "metadata": {
        "id": "0vnr4arksRNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor():\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        weights\n",
        "    ):\n",
        "        model = model(weights = weights)\n",
        "        model.eval()\n",
        "        self.model = model\n",
        "        self.preprocessing = weights.transforms()\n",
        "        nodes, _ = get_graph_node_names(self.model)\n",
        "        self.nodes = nodes\n",
        "\n",
        "    def extract(\n",
        "        self,\n",
        "        layer_idx,\n",
        "        model_input\n",
        "    ):\n",
        "        return_node_name = self.nodes[layer_idx]\n",
        "        return_nodes = {return_node_name: return_node_name}\n",
        "        extractor = create_feature_extractor(\n",
        "            self.model,\n",
        "            return_nodes=return_nodes\n",
        "        )\n",
        "        preprocessed_input = self.preprocessing(model_input)\n",
        "        features = extractor(preprocessed_input)\n",
        "        node_features = features[return_node_name]\n",
        "\n",
        "        return node_features.clone().detach()"
      ],
      "metadata": {
        "id": "N-KSeZYvsP-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions for data preparation and augmentation"
      ],
      "metadata": {
        "id": "fVWejmXiY-Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following method was taken from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "\n",
        "\n",
        "def crop_center_square(frame):\n",
        "    '''\n",
        "    crop captured frame such that\n",
        "    resulting image has square shape\n",
        "    '''\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
        "\n",
        "def load_video(path, max_frames=24, img_size=(128, 128)):\n",
        "    video_frames = []\n",
        "    cap = cv.VideoCapture(path)\n",
        "    try:\n",
        "        while True:\n",
        "            x, frame = cap.read()\n",
        "            if not x:\n",
        "                break\n",
        "            frame = crop_center_square(frame)\n",
        "            frame = cv.resize(frame, img_size)\n",
        "            #swap color channels\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            video_frames.append(frame)\n",
        "\n",
        "            if len(video_frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "\n",
        "    na = np.array(video_frames)\n",
        "    tt = torch.tensor(na)/255.\n",
        "    tt = tt.permute(0, 3, 1, 2)\n",
        "\n",
        "    return tt"
      ],
      "metadata": {
        "id": "d1azXHYdG8Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_labels(df):\n",
        "    unique_labels = list(df['tag'].unique())\n",
        "    label_lookup = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded_labels = torch.tensor(np.array(list(map(lambda l: label_lookup[l],\n",
        "                                                    df['tag']))))\n",
        "\n",
        "    return encoded_labels\n",
        "\n",
        "def extract_and_pad(video_frames, fx, transform=None):\n",
        "    if transform is not None:\n",
        "        video_frames = transform(video_frames)\n",
        "\n",
        "    video_features = fx.extract(-2, video_frames)\n",
        "\n",
        "    #add padding where necessary\n",
        "    if video_features.shape[0] < max_video_length:\n",
        "        diff = max_video_length - video_features.shape[0]\n",
        "        padding = torch.zeros((diff, num_features))\n",
        "        video_features = torch.cat((video_features, padding))\n",
        "\n",
        "    video_features = video_features.unsqueeze(0)\n",
        "\n",
        "    return video_features\n",
        "\n",
        "def prepare_train_samples(df, root_dir, fx, transform=None):\n",
        "    labels = encode_labels(df)\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "\n",
        "    all_frame_features1 = torch.zeros(\n",
        "        (num_samples, max_video_length, num_features)\n",
        "    )\n",
        "\n",
        "    all_frame_features2 = torch.zeros(\n",
        "        (num_samples, max_video_length, num_features)\n",
        "    )\n",
        "\n",
        "    for idx, path in enumerate(video_paths):\n",
        "\n",
        "        frames = load_video(os.path.join(root_dir, path),\n",
        "                            img_size=(image_size, image_size))\n",
        "\n",
        "        video_features_org = extract_and_pad(frames, fx)\n",
        "        all_frame_features1[idx,] = video_features_org\n",
        "\n",
        "        video_features_trans = extract_and_pad(frames, fx, transform=transform)\n",
        "        all_frame_features2[idx,] = video_features_trans\n",
        "\n",
        "    upsampled_data = torch.cat((all_frame_features1, all_frame_features2))\n",
        "    upsampled_labels = torch.cat((labels, labels))\n",
        "\n",
        "    return upsampled_data, upsampled_labels\n",
        "\n",
        "def prepare_test_samples(df, root_dir, fx):\n",
        "    labels = encode_labels(df)\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "\n",
        "    all_frame_features = torch.zeros(\n",
        "        (num_samples, max_video_length, num_features)\n",
        "    )\n",
        "\n",
        "    for idx, path in enumerate(video_paths):\n",
        "\n",
        "        frames = load_video(os.path.join(root_dir, path),\n",
        "                            img_size=(image_size, image_size))\n",
        "\n",
        "        video_features = extract_and_pad(frames, fx)\n",
        "        all_frame_features[idx,] = video_features\n",
        "\n",
        "    return all_frame_features, labels"
      ],
      "metadata": {
        "id": "iWaO-leQk5Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build custom datasets for training and validation\n",
        "\n",
        "The following steps are performed for all samples:\n",
        "\n",
        "1. label encoding\n",
        "\n",
        "2. capture of up to 24 frames of the video\n",
        "\n",
        "3. extraction of frame features using the DenseNet121 model\n",
        "\n",
        "4. padding where video length < 24 frames\n",
        "\n",
        "Furthermore, data augmentation is performed by producing a grayscale copy of each training sample, doubling the size of the training dataset."
      ],
      "metadata": {
        "id": "dA6cACTh3V4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SubsampledActionDataset(Dataset):\n",
        "    def __init__(self, dataset_type, fx, transform=None):\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        if dataset_type == \"test\":\n",
        "            df = pd.read_csv(\"test.csv\")\n",
        "            self.data, self.labels = prepare_test_samples(\n",
        "                df,\n",
        "                dataset_type,\n",
        "                fx\n",
        "            )\n",
        "        else:\n",
        "            df = pd.read_csv(\"train.csv\")\n",
        "            self.data, self.labels = prepare_train_samples(\n",
        "                df,\n",
        "                dataset_type,\n",
        "                fx,\n",
        "                transform=self.transform,\n",
        "            )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vid_features = self.data[idx]\n",
        "        vid_label = self.labels[idx]\n",
        "\n",
        "        sample = {'video': vid_features, 'action': vid_label}\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "YBsnTrgn3Nw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!!wget -q https://github.com/sayakpaul/Action-Recognition-in-TensorFlow/releases/download/v1.0.0/ucf101_top5.tar.gz\n",
        "#!tar xf ucf101_top5.tar.gz\n",
        "\n",
        "net_fx = FeatureExtractor(densenet121, DenseNet121_Weights.IMAGENET1K_V1)\n",
        "\n",
        "action_dataset_train = SubsampledActionDataset(\n",
        "    \"train\",\n",
        "    net_fx,\n",
        "    transform=v2.Grayscale(num_output_channels=3)\n",
        ")\n",
        "\n",
        "action_dataset_val = SubsampledActionDataset(\n",
        "    \"test\",\n",
        "    net_fx\n",
        ")"
      ],
      "metadata": {
        "id": "fUvy2Asbibo_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "112eac0b-ec2e-4f80-e701-9714dc0a34e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 105MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(action_dataset_train, '/content/drive/My Drive/video_train.pt')\n",
        "torch.save(action_dataset_val, '/content/drive/My Drive/video_val.pt')"
      ],
      "metadata": {
        "id": "iJgKvATbz04Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_dataset_train = torch.load('/content/drive/My Drive/video_train.pt', weights_only=False)\n",
        "action_dataset_val = torch.load('/content/drive/My Drive/video_val.pt', weights_only=False)\n",
        "train_dataloader = DataLoader(action_dataset_train, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(action_dataset_val, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "jVI6nTIvX7gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build video classifier with TransformerEncoder module and RoPE\n",
        "\n",
        "The features extracted from the video frames are fed into a TransformerEncoder module with Rotary Positional Embeddings. Global average pooling is performed on the encoder outputs before predictions are generated by the linear output layer."
      ],
      "metadata": {
        "id": "bBu3E2DGOHI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.d_model = d_model # Model's dimension\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
        "\n",
        "        # Linear layers for transforming inputs\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
        "\n",
        "        #Rotary Positional Embeddings\n",
        "        self.rotary_pos_emb = RotaryPositionalEmbeddings(self.d_k)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def apply_rotation(self, v):\n",
        "        rotary_pos_emb = self.rotary_pos_emb(v)\n",
        "        return rotary_pos_emb\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask, -1e9)\n",
        "\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        Q = self.split_heads(self.W_q(x))\n",
        "        K = self.split_heads(self.W_k(x))\n",
        "        V = self.split_heads(self.W_v(x))\n",
        "\n",
        "        #Encode positions\n",
        "        Q = self.apply_rotation(Q)\n",
        "        K = self.apply_rotation(K)\n",
        "\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Combine heads and apply output transformation\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.device = torch.accelerator.current_accelerator().type \\\n",
        "                      if torch.accelerator.is_available() else \"cpu\"\n",
        "        self.num_heads = num_heads\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_padding_mask(self, x):\n",
        "        padding_mask = (x[:, :, 0] == 0).unsqueeze(1).unsqueeze(2)\n",
        "        padding_mask = padding_mask.to(self.device)\n",
        "        return padding_mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        padding_mask = self.generate_padding_mask(x)\n",
        "        attention_output = self.self_attention(x, mask=padding_mask)\n",
        "        x = self.norm1(x + self.dropout(attention_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "class ClassifierModule(nn.Module):\n",
        "    def __init__(self, num_classes, num_features, cl_dropout):\n",
        "        super(ClassifierModule, self).__init__()\n",
        "        self.output_layer = nn.Linear(num_features, num_classes)\n",
        "        self.cl_dropout = nn.Dropout(cl_dropout)\n",
        "\n",
        "    def global_pool(self, x):\n",
        "        pooled_x = torch.mean(x, 1)\n",
        "        return pooled_x\n",
        "\n",
        "    def forward(self, x):\n",
        "        pooled_x = self.global_pool(x)\n",
        "        dropout = self.cl_dropout(pooled_x)\n",
        "        output = self.output_layer(dropout)\n",
        "        return output\n",
        "\n",
        "class VideoClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        num_features,\n",
        "        d_model,\n",
        "        num_heads,\n",
        "        d_ff,\n",
        "        dropout,\n",
        "        cl_dropout\n",
        "    ):\n",
        "        super(VideoClassifier, self).__init__()\n",
        "\n",
        "        self.components = nn.Sequential(\n",
        "            TransformerEncoder(d_model, num_heads, d_ff, dropout),\n",
        "            ClassifierModule(num_classes, num_features, cl_dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.components(x)"
      ],
      "metadata": {
        "id": "NoFFxmpXWqKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = 1024\n",
        "d_model = num_features\n",
        "seq_length = 24\n",
        "num_classes = 5\n",
        "num_heads = 4\n",
        "d_ff = 200 #300\n",
        "dropout = 0.1\n",
        "cl_dropout = 0.2\n",
        "video_model = VideoClassifier(\n",
        "    num_classes,\n",
        "    num_features,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    d_ff,\n",
        "    dropout,\n",
        "    cl_dropout\n",
        ")\n",
        "device = torch.accelerator.current_accelerator().type \\\n",
        "                      if torch.accelerator.is_available() else \"cpu\"\n",
        "video_model = video_model.to(device)"
      ],
      "metadata": {
        "id": "-_H2fkygWphJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training\n",
        "\n",
        "The model is trained for 4 epochs with the Adam optimizer (default settings), using a batch size of 16 and a scheduler that reduces the learning rate when the validation loss fails to improve for more than 2 consecutive epochs."
      ],
      "metadata": {
        "id": "rJiy2-GTex1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(video_model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
        "\n",
        "def train(model, dataloader, loss_func, optimizer):\n",
        "    num_samples = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        batch_X, batch_y = batch['video'].to(device), batch['action'].to(device)\n",
        "        pred = model(batch_X)\n",
        "        loss = loss_func(pred, batch_y)\n",
        "\n",
        "        #backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #print progress\n",
        "        if batch_idx % 10 == 0:\n",
        "            loss, sample_idx = loss.item(), (batch_idx + 1) * len(batch_X)\n",
        "            print(f\"training loss (batch {batch_idx}): {loss} [sample {sample_idx}/{num_samples}]\")\n",
        "\n",
        "def validate(model, dataloader, loss_func):\n",
        "    num_batches = len(dataloader)\n",
        "    num_samples = len(dataloader.dataset)\n",
        "    model.eval()\n",
        "    val_loss, correct_pred = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for val_batch in dataloader:\n",
        "            X, y = val_batch['video'].to(device), val_batch['action'].to(device)\n",
        "            pred = model(X)\n",
        "            val_loss+= loss_func(pred, y).item()\n",
        "            correct_pred+= (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    val_loss /= num_batches\n",
        "    correct_pred /= num_samples\n",
        "    print(f\"Validation results for epoch: \\n Accuracy: {(100*correct_pred):>0.1f}%, \" \\\n",
        "          f\"Avg. per-batch loss: {val_loss:>8f} \\n\")\n",
        "\n",
        "    return val_loss\n"
      ],
      "metadata": {
        "id": "29pbBw_LyIVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(4):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    train(video_model, train_dataloader, loss_func, optimizer)\n",
        "    val_loss = validate(video_model, val_dataloader, loss_func)\n",
        "    #update learning rate\n",
        "    scheduler.step(val_loss)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjRcPbK9RY0F",
        "outputId": "794820f4-a7bc-404e-e2d0-68bef458911b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "training loss (batch 0): 1.713446021080017 [sample 16/1188]\n",
            "training loss (batch 10): 0.0420900322496891 [sample 176/1188]\n",
            "training loss (batch 20): 0.09822942316532135 [sample 336/1188]\n",
            "training loss (batch 30): 0.05676614120602608 [sample 496/1188]\n",
            "training loss (batch 40): 0.14386054873466492 [sample 656/1188]\n",
            "training loss (batch 50): 0.00016213249182328582 [sample 816/1188]\n",
            "training loss (batch 60): 0.010039865970611572 [sample 976/1188]\n",
            "training loss (batch 70): 0.002075702417641878 [sample 1136/1188]\n",
            "Validation results for epoch: \n",
            " Accuracy: 98.2%, Avg. per-batch loss: 0.066161 \n",
            "\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "training loss (batch 0): 0.0030528870411217213 [sample 16/1188]\n",
            "training loss (batch 10): 0.0014314644504338503 [sample 176/1188]\n",
            "training loss (batch 20): 0.0005880095413886011 [sample 336/1188]\n",
            "training loss (batch 30): 0.0005521089187823236 [sample 496/1188]\n",
            "training loss (batch 40): 0.00016838934971019626 [sample 656/1188]\n",
            "training loss (batch 50): 0.00028417378780432045 [sample 816/1188]\n",
            "training loss (batch 60): 0.0004253906081430614 [sample 976/1188]\n",
            "training loss (batch 70): 0.0001300268340855837 [sample 1136/1188]\n",
            "Validation results for epoch: \n",
            " Accuracy: 98.7%, Avg. per-batch loss: 0.034969 \n",
            "\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "training loss (batch 0): 2.3028653231449425e-05 [sample 16/1188]\n",
            "training loss (batch 10): 3.4419917938066646e-05 [sample 176/1188]\n",
            "training loss (batch 20): 7.147071301005781e-05 [sample 336/1188]\n",
            "training loss (batch 30): 4.1870385757647455e-05 [sample 496/1188]\n",
            "training loss (batch 40): 7.361704047070816e-05 [sample 656/1188]\n",
            "training loss (batch 50): 5.4324293159879744e-05 [sample 816/1188]\n",
            "training loss (batch 60): 5.956931636319496e-05 [sample 976/1188]\n",
            "training loss (batch 70): 1.708360287011601e-05 [sample 1136/1188]\n",
            "Validation results for epoch: \n",
            " Accuracy: 98.7%, Avg. per-batch loss: 0.030031 \n",
            "\n",
            "\n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "training loss (batch 0): 2.369193134654779e-05 [sample 16/1188]\n",
            "training loss (batch 10): 3.5863995435647666e-05 [sample 176/1188]\n",
            "training loss (batch 20): 8.880956556822639e-06 [sample 336/1188]\n",
            "training loss (batch 30): 5.026560393162072e-05 [sample 496/1188]\n",
            "training loss (batch 40): 1.583181438036263e-05 [sample 656/1188]\n",
            "training loss (batch 50): 1.5884341337368824e-05 [sample 816/1188]\n",
            "training loss (batch 60): 8.977820471045561e-06 [sample 976/1188]\n",
            "training loss (batch 70): 2.0085981304873712e-05 [sample 1136/1188]\n",
            "Validation results for epoch: \n",
            " Accuracy: 99.1%, Avg. per-batch loss: 0.028852 \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> The model achieves a maximum validation accuracy of 99.1%, which doesn't seem too bad."
      ],
      "metadata": {
        "id": "pdb_h9GZDyV-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HkuWIajdEDyc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}